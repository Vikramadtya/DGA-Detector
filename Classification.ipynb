{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ4rv0Czd10Y"
      },
      "source": [
        "# Classification\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0qs3vJo4R_G"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "PROCESSED_AGD_DATASET = pd.read_csv('PROCESSED_AGD_DATASET.csv')\n",
        "PROCESSED_BENIGN_DATASET = pd.read_csv('PROCESSED_BENIGN_DATASET.csv')\n",
        "\n",
        "AGD = ['banjori','dnschanger','fobber','kraken','monerodownloader','murofret','necurs','newgoz','nymaim','padcrypt','pushdo','pyskpa','qadars','ranbyus','shiotob','simda','symmi','zloader']\n",
        "\n",
        "DATAFRAME_AGD = {}\n",
        "BALANCED_DATAFRAME_AGD = {}\n",
        "BALANCED_DATAFRAME_BENIGN = None\n",
        "\n",
        "FINAL_DATAFRAME_AGD = None\n",
        "FINAL_DATAFRAME_BENIGN = None\n",
        "\n",
        "FINAL_DATAFRAME = None\n",
        "\n",
        "FEATURE_LABELS = None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnCvC9SPvSRQ"
      },
      "source": [
        "# !apt install enchant\n",
        "# !pip install pyenchant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1lemtqF2Aup"
      },
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# separate observations from each class into different DataFrames\n",
        "\n",
        "for x in AGD:\n",
        "  DATAFRAME_AGD[x] = PROCESSED_AGD_DATASET[PROCESSED_AGD_DATASET.agd_type ==x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I1ANn9M7G8R"
      },
      "source": [
        "# balance the AGD's\n",
        "y = 1000\n",
        "\n",
        "for x in AGD:\n",
        "  try:\n",
        "    BALANCED_DATAFRAME_AGD[x] = resample(DATAFRAME_AGD[x],replace=False,n_samples=y)\n",
        "  except:\n",
        "    BALANCED_DATAFRAME_AGD[x] = resample(DATAFRAME_AGD[x],replace=True,n_samples=y)\n",
        "\n",
        "\n",
        "# balance the benign\n",
        "BALANCED_DATAFRAME_BENIGN = resample(PROCESSED_BENIGN_DATASET,replace=False,n_samples= len(AGD)*y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xsoJTMJo_9pq"
      },
      "source": [
        " # AGD DATAFRAME CONCATED\n",
        " FINAL_DATAFRAME_AGD = pd.concat( list(BALANCED_DATAFRAME_AGD.values()) )\n",
        " FINAL_DATAFRAME_BENIGN = BALANCED_DATAFRAME_BENIGN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqIVUsyRA1CE"
      },
      "source": [
        "FINAL_DATAFRAME = pd.concat([FINAL_DATAFRAME_AGD,FINAL_DATAFRAME_BENIGN])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqEhkvIPAlsI"
      },
      "source": [
        "LABEL_ARRAY = np.array(FINAL_DATAFRAME.domain_class)\n",
        "FEATURE_ARRAY = np.asarray(FINAL_DATAFRAME.iloc[:,2:].values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WKLswrVgB8XO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "05ec4802-1dee-4add-d18b-7035f261e1bc"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "TRAIN_FEATURES,TEST_FEATURES,TRAIN_LABELS,TEST_LABELS = train_test_split(FEATURE_ARRAY,LABEL_ARRAY,test_size = 0.2,random_state = 42)\n",
        "\n",
        "print('Training Features Shape:', TRAIN_FEATURES.shape)\n",
        "print('Training Labels Shape:', TRAIN_LABELS.shape)\n",
        "print('Testing Features Shape:', TEST_FEATURES.shape)\n",
        "print('Testing Labels Shape:', TEST_LABELS.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Features Shape: (28800, 16)\n",
            "Training Labels Shape: (28800,)\n",
            "Testing Features Shape: (7200, 16)\n",
            "Testing Labels Shape: (7200,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IR9OsUPxCrey",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "837bf121-e3ef-482a-ff34-99c939b87e21"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "classifier = RandomForestClassifier(n_estimators = 2500, random_state = 42)\n",
        "classifier.fit(TRAIN_FEATURES,TRAIN_LABELS)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
              "                       criterion='gini', max_depth=None, max_features='auto',\n",
              "                       max_leaf_nodes=None, max_samples=None,\n",
              "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
              "                       min_samples_leaf=1, min_samples_split=2,\n",
              "                       min_weight_fraction_leaf=0.0, n_estimators=2500,\n",
              "                       n_jobs=None, oob_score=False, random_state=42, verbose=0,\n",
              "                       warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bL9VGV5vStw_",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "6d8bce8b-4df8-482e-cd5d-f80943e14223"
      },
      "source": [
        "FEATURE_LABELS = list(FINAL_DATAFRAME_AGD.columns)[2:]\n",
        "for feature in zip(FEATURE_LABELS, classifier.feature_importances_):\n",
        "    print(feature)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "('domain_len', 0.13262987525515665)\n",
            "('vowel_count_to_domain_length', 0.014816314838060467)\n",
            "('digit_count_to_domain_length', 0.0038526989145963755)\n",
            "('repeat_character_count_to_domain_length', 0.024813634099783762)\n",
            "('shanon_entropy', 0.0896919470351042)\n",
            "('number_of_meaningfull_substring_to_domain_length', 0.0407084100332614)\n",
            "('longest_meaningfull_substring_to_domain_length', 0.019512802656065174)\n",
            "('consecutive_digit_ratio', 0.14050869011665063)\n",
            "('entropy_relative_to_en_words', 0.012791155491739056)\n",
            "('entropy_relative_to_en_domain', 0.018302858873758827)\n",
            "('bi_gram_score_relative_to_en_domains', 0.11252048918180244)\n",
            "('tri_gram_score_relative_to_en_domains', 0.1263748684232117)\n",
            "('quad_gram_score_relative_to_en_domain', 0.1434532785279071)\n",
            "('bi_gram_score_relative_to_en_words', 0.05268663871032726)\n",
            "('tri_gram_score_relative_to_en_words', 0.041383328278917116)\n",
            "('quad_gram_score_relative_to_en_words', 0.02595300956365801)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PvCR4OhUH_R"
      },
      "source": [
        "pred = classifier.predict(TEST_FEATURES)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9GnI6n3OUQeF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a24ac392-52ce-4229-8cd0-ba017089353b"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(TEST_LABELS, pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.97875"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vl-7WUFXhkSZ"
      },
      "source": [
        "AUC is desirable for the following two reasons:\n",
        "\n",
        "AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
        "AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vuCEbwY_fcVy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "80657502-ada7-461f-bb21-5d7edcb11e75"
      },
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "prob_y_2 = classifier.predict_proba(TEST_FEATURES)\n",
        "prob_y_2 = [p[1] for p in prob_y_2]\n",
        "print(\"AUC score :\", roc_auc_score(TEST_LABELS, prob_y_2) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "AUC score : 0.9977988038425003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5hQhmYgWtJql"
      },
      "source": [
        "# PRINT A TREE FROM CLASSIFIER\n",
        "\n",
        "# Import tools needed for visualization\n",
        "\n",
        "from sklearn.tree import export_graphviz\n",
        "import pydot\n",
        "\n",
        "# Pull out one tree from the forest\n",
        "\n",
        "tree = classifier.estimators_[5]\n",
        "\n",
        "# Export the image to a dot file\n",
        "\n",
        "export_graphviz(tree, out_file = 'tree.dot', feature_names = FEATURE_LABELS, rounded = True, precision = 1)\n",
        "\n",
        "# Use dot file to create a graph\n",
        "\n",
        "(graph, ) = pydot.graph_from_dot_file('tree.dot')\n",
        "\n",
        "# Write graph to a png file\n",
        "graph.write_png('tree.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TioOykQGCWzV"
      },
      "source": [
        "# API"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeTPDwdiCyjX"
      },
      "source": [
        "# # # !mv *.csv ./DATASET/AGD/\n",
        "# !pip install pyenchant\n",
        "# !apt install enchant"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L_1d5vZLCVxY"
      },
      "source": [
        "# FILE 1 : GLOBAL_VARIABLES\n",
        "from collections import Counter\n",
        "\n",
        "CLEANED_AGD_DATASET = None\n",
        "CLEANED_BENIGN_DATASET = None\n",
        "\n",
        "PROCESSED_AGD_DATASET = None\n",
        "PROCESSED_BENIGN_DATASET = None\n",
        "\n",
        "DATASET_LOCATION = '/content/DATASET'\n",
        "\n",
        "PARSED_DOMAIN = {}\n",
        "\n",
        "CHARACTER_IN_DOMAIN = {}\n",
        "\n",
        "CHARACTER_DISTRIBUTION_ENGLISH_WORDS = Counter({'total':0})\n",
        "CHARACTER_DISTRIBUTION_ENGLISH_DOMAINS = Counter({'total':0})\n",
        "\n",
        "DOMAIN_BIGRAM_DISTRIBUTION = Counter({'total':0})\n",
        "DOMAIN_TRIGRAM_DISTRIBUTION = Counter({'total':0})\n",
        "DOMAIN_QUADGRAM_DISTRIBUTION = Counter({'total':0})\n",
        "\n",
        "ENGLISH_WORD_BIGRAM_DISTRIBUTION = Counter({'total':0})\n",
        "ENGLISH_WORD_TRIGRAM_DISTRIBUTION = Counter({'total':0})\n",
        "ENGLISH_WORD_QUADGRAM_DISTRIBUTION = Counter({'total':0})\n",
        "\n",
        "DOMAIN_MEANINGFULL_STRING = {}\n",
        "VOWELS = ['a','e','i','o','u']\n",
        "\n",
        "FEATURES = {}\n",
        "\n",
        "DATASETS = {'AGD':{},'BENIGN':{},'EN_WORDS':{},'TEST':{}}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8NZYcsdYiR_j"
      },
      "source": [
        "# FILE 2 : DOMAIN PARSING\n",
        "\n",
        "# Parse a domain name\n",
        "def parse_domain(domain):\n",
        "    domain_splitted = domain.split('.')\n",
        "    domain_level = {}\n",
        "    i = 1\n",
        "    while domain_splitted:\n",
        "        domain_level[i] = domain_splitted.pop().lower()\n",
        "        i += 1\n",
        "    return domain_level\n",
        "\n",
        "def get_parsed_domain(domain):\n",
        "  if domain not in PARSED_DOMAIN:\n",
        "    PARSED_DOMAIN[domain]= parse_domain(domain)\n",
        "  return PARSED_DOMAIN[domain]\n",
        "\n",
        "def get_top_level_domain(domain):\n",
        "  return get_parsed_domain(domain)[1]\n",
        "\n",
        "def get_second_level_domain(domain):\n",
        "  return get_parsed_domain(domain)[2]\n",
        "\n",
        "def is_parsed(domain):\n",
        "  return domain in PARSED_DOMAIN\n",
        "\n",
        "# Test\n",
        "# print(get_parsed_domain('www.fb.gh.com'))\n",
        "# print(get_parsed_domain('www.lg.mt.trtsfd.in'))\n",
        "# print(get_top_level_domain('www.fb.gh.com'))\n",
        "# print(get_second_level_domain('www.fb.gh.com'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEJnkn4RBJyD"
      },
      "source": [
        "# FILE 3 : MAIN FUNCTIONS\n",
        "\n",
        "import pandas as pd\n",
        "import enchant\n",
        "dictionary = enchant.Dict(\"en_US\")\n",
        "\n",
        "# CHARACTER COUNT OF A DOMAIN\n",
        "def characters_in_domain(domain):\n",
        "  if domain not in CHARACTER_IN_DOMAIN:\n",
        "    CHARACTER_IN_DOMAIN[domain] = Counter(domain)\n",
        "  return CHARACTER_IN_DOMAIN[domain]\n",
        "\n",
        "# CREATE A DISTRIBUTION\n",
        "\n",
        "def add_to_distribution(domain,distribution):\n",
        "  distribution += CHARACTER_IN_DOMAIN[domain]\n",
        "  distribution['total'] += sum([y for x,y in dict(CHARACTER_IN_DOMAIN[domain]).items()])\n",
        "\n",
        "# LIST N-GRAM OF STRING\n",
        "def n_gram(string, n):\n",
        "    string_len = len(string)\n",
        "    return [string[i:i+n] for i in range(string_len) if i+n <= string_len ]\n",
        "\n",
        "# ADD STRING'S N-GRAM TO DISTRIBUTION\n",
        "def add_grams(string,isDomain):\n",
        "  global DOMAIN_BIGRAM_DISTRIBUTION,DOMAIN_TRIGRAM_DISTRIBUTION,DOMAIN_QUADGRAM_DISTRIBUTION,ENGLISH_WORD_BIGRAM_DISTRIBUTION,ENGLISH_WORD_TRIGRAM_DISTRIBUTION,ENGLISH_WORD_QUADGRAM_DISTRIBUTION\n",
        "  if isDomain:\n",
        "    DOMAIN_BIGRAM_DISTRIBUTION += Counter(n_gram(string, 2))\n",
        "    DOMAIN_TRIGRAM_DISTRIBUTION += Counter(n_gram(string, 3))\n",
        "    DOMAIN_QUADGRAM_DISTRIBUTION += Counter(n_gram(string, 4))\n",
        "\n",
        "  if not isDomain:\n",
        "    ENGLISH_WORD_BIGRAM_DISTRIBUTION += Counter(n_gram(string, 2))\n",
        "    ENGLISH_WORD_TRIGRAM_DISTRIBUTION += Counter(n_gram(string, 3))\n",
        "    ENGLISH_WORD_QUADGRAM_DISTRIBUTION += Counter(n_gram(string, 4))\n",
        "\n",
        "# LIST ALL SUB-STRING OF A STRING\n",
        "def generate_sub_strings(string):\n",
        "  y = []\n",
        "  string_len = len(string)\n",
        "  for n in range(3,string_len):\n",
        "    y.extend([string[i:i+n] for i in range(string_len) if i+n <= string_len])\n",
        "  return y\n",
        "\n",
        "# CHECK IF MEANING-FULL\n",
        "def isMeaningfull(string):\n",
        "  return dictionary.check(string)\n",
        "\n",
        "# EXTRACT FEATURE FROM A DOMAIN\n",
        "def extract_features(domain):\n",
        "  return [x(domain) for y,x in FEATURES.items()]\n",
        "\n",
        "# ADD ADDITIONAL LABEL TO FEATURE'S DATAFRAME\n",
        "def label_feature(features,domain_class='benign',agd_type='benign'):\n",
        "  return [domain_class,agd_type]+features\n",
        "\n",
        "# SAVE A DATA FRAME TO DIRECTORY\n",
        "def save_dataframe(dataframe,name):\n",
        "  dataframe.to_csv('.'.join([name,'csv']),index=False)\n",
        "\n",
        "# GENERATE ALL FEATURES FOR THE DATASET\n",
        "def generate_features(dataset):\n",
        "    _ = []\n",
        "    for index, row in dataset.iterrows():\n",
        "      _.append(label_feature(extract_features(row['domain']),row['domain_class'],row['agd_type']))\n",
        "    return pd.DataFrame( _, columns=['domain_class','agd_type']+list(FEATURES.keys()) )\n",
        "\n",
        "\n",
        "# DIVIDE A VALUE BY LENGTH OF DOMAIN\n",
        "def divide_by_domain_len(value,domain):\n",
        "  return value/len(domain)\n",
        "#\n",
        "def relative_entropy(domain,distribution):\n",
        "  x = symbol_probability(characters_in_domain(domain))\n",
        "  try:\n",
        "    return sum([  -1*y*log2(y/(distribution[i]/distribution['total']))  for i,y in x.items() ])\n",
        "  except:\n",
        "    print([(i,distribution[i],distribution['total'])  for i,y in x.items() ])\n",
        "    return 0\n",
        "\n",
        "#\n",
        "def symbol_probability(freq):\n",
        "  total_characters = sum(freq.values())\n",
        "  return {x:freq[x]/total_characters for x in freq}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6GpYh3uBRYF"
      },
      "source": [
        "# FILE 4 : FEATURES\n",
        "\n",
        "from math import log2\n",
        "\n",
        "def domain_len(domain):\n",
        "  x = characters_in_domain(domain)\n",
        "  return sum(x.values())\n",
        "\n",
        "def vowel_count_to_domain_length(domain):\n",
        "  vowels = sum([y for x,y in dict(characters_in_domain(domain)).items() if x in VOWELS ])\n",
        "  domain_len = len(domain)\n",
        "  return vowels/domain_len\n",
        "\n",
        "def digit_count_to_domain_length(domain):\n",
        "  digits = sum([y for x,y in dict(characters_in_domain(domain)).items() if x.isdigit() ])\n",
        "  domain_len = len(domain)\n",
        "  return digits/domain_len\n",
        "\n",
        "def repeat_character_count_to_domain_length(domain):\n",
        "  repeated = sum([y for x,y in dict(characters_in_domain(domain)).items() if y > 1 ])\n",
        "  domain_len = len(domain)\n",
        "  return repeated/domain_len\n",
        "\n",
        "def shanon_entropy(domain):\n",
        "  x = symbol_probability(characters_in_domain(domain))\n",
        "  return sum([ -1*y*log2(y) for y in x.values() ])\n",
        "\n",
        "\n",
        "def number_of_meaningfull_substring_to_domain_length(domain):\n",
        "  if domain not in DOMAIN_MEANINGFULL_STRING:\n",
        "    DOMAIN_MEANINGFULL_STRING[domain] = [ x for x in generate_sub_strings(domain) if isMeaningfull(x)]\n",
        "  return len(DOMAIN_MEANINGFULL_STRING[domain])/len(domain)\n",
        "\n",
        "def longest_meaningfull_substring_to_domain_length(domain):\n",
        "  if domain not in DOMAIN_MEANINGFULL_STRING:\n",
        "    DOMAIN_MEANINGFULL_STRING[domain] = [ x for x in generate_sub_strings(domain) if isMeaningfull(x)]\n",
        "  return max(list(map(len,DOMAIN_MEANINGFULL_STRING[domain]+[''])))\n",
        "\n",
        "def entropy_relative_to_en_words(domain):\n",
        "  return relative_entropy(domain,CHARACTER_DISTRIBUTION_ENGLISH_WORDS)\n",
        "\n",
        "def entropy_relative_to_en_domain(domain):\n",
        "  return relative_entropy(domain,CHARACTER_DISTRIBUTION_ENGLISH_DOMAINS)\n",
        "\n",
        "def bi_gram_score_relative_to_en_domains(domain):\n",
        "  x = Counter(n_gram(domain,2))\n",
        "  return divide_by_domain_len(sum( [x[c]*DOMAIN_BIGRAM_DISTRIBUTION[c]  for c in x] ),domain)\n",
        "\n",
        "def tri_gram_score_relative_to_en_domains(domain):\n",
        "  x = Counter(n_gram(domain,3))\n",
        "  return divide_by_domain_len(sum( [x[c]*DOMAIN_TRIGRAM_DISTRIBUTION[c]  for c in x] ),domain)\n",
        "\n",
        "def quad_gram_score_relative_to_en_domain(domain):\n",
        "  x = Counter(n_gram(domain,4))\n",
        "  return divide_by_domain_len(sum( [x[c]*DOMAIN_QUADGRAM_DISTRIBUTION[c]  for c in x] ),domain)\n",
        "\n",
        "def bi_gram_score_relative_to_en_words(domain):\n",
        "  x = Counter(n_gram(domain,2))\n",
        "  return divide_by_domain_len(sum( [x[c]*ENGLISH_WORD_BIGRAM_DISTRIBUTION[c]  for c in x] ),domain)\n",
        "\n",
        "def tri_gram_score_relative_to_en_words(domain):\n",
        "  x = Counter(n_gram(domain,3))\n",
        "  return divide_by_domain_len(sum( [x[c]*ENGLISH_WORD_TRIGRAM_DISTRIBUTION[c]  for c in x] ),domain)\n",
        "\n",
        "def quad_gram_score_relative_to_en_words(domain):\n",
        "  x = Counter(n_gram(domain,4))\n",
        "  return divide_by_domain_len(sum( [x[c]*ENGLISH_WORD_QUADGRAM_DISTRIBUTION[c]  for c in x] ),domain)\n",
        "\n",
        "def consecutive_digit_ratio(domain):\n",
        "  x = Counter(n_gram('aaaa',2))\n",
        "  return divide_by_domain_len(sum([y for g,y in x.items() if g[0]==g[1]]),domain)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FigqlxHBdZh"
      },
      "source": [
        "# ADDING FEATURES\n",
        "FEATURES['domain_len'] = domain_len\n",
        "FEATURES['vowel_count_to_domain_length'] = vowel_count_to_domain_length\n",
        "FEATURES['digit_count_to_domain_length'] = digit_count_to_domain_length\n",
        "FEATURES['repeat_character_count_to_domain_length'] = repeat_character_count_to_domain_length\n",
        "\n",
        "FEATURES['shanon_entropy'] = shanon_entropy\n",
        "\n",
        "FEATURES['number_of_meaningfull_substring_to_domain_length'] = number_of_meaningfull_substring_to_domain_length\n",
        "FEATURES['longest_meaningfull_substring_to_domain_length'] = longest_meaningfull_substring_to_domain_length\n",
        "\n",
        "FEATURES['consecutive_digit_ratio']=consecutive_digit_ratio\n",
        "\n",
        "FEATURES['entropy_relative_to_en_words']=entropy_relative_to_en_words\n",
        "FEATURES['entropy_relative_to_en_domain']=entropy_relative_to_en_domain\n",
        "\n",
        "FEATURES['bi_gram_score_relative_to_en_domains'] = bi_gram_score_relative_to_en_domains\n",
        "FEATURES['tri_gram_score_relative_to_en_domains'] = tri_gram_score_relative_to_en_domains\n",
        "FEATURES['quad_gram_score_relative_to_en_domain'] = quad_gram_score_relative_to_en_domain\n",
        "\n",
        "FEATURES['bi_gram_score_relative_to_en_words'] = bi_gram_score_relative_to_en_words\n",
        "FEATURES['tri_gram_score_relative_to_en_words'] = tri_gram_score_relative_to_en_words\n",
        "FEATURES['quad_gram_score_relative_to_en_words'] = quad_gram_score_relative_to_en_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5X7YoaYTBeX-"
      },
      "source": [
        "# FILE 5: LOAD DATASETS\n",
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "def find_and_load_dataset():\n",
        "  def is_hidden(file):\n",
        "    return True if file[0] == '.' else False\n",
        "\n",
        "  def load_dataset(directory,class_of):\n",
        "    with os.scandir(directory) as listing:\n",
        "      for file in listing:\n",
        "        if file.is_file():\n",
        "          if not is_hidden(file.name):\n",
        "            DATASETS[class_of][file.name.split('.')[0]] =  pd.read_csv(os.path.join(directory,file.name))[:50000]\n",
        "\n",
        "  with os.scandir(DATASET_LOCATION) as listing:\n",
        "    for file in listing:\n",
        "      if file.is_dir():\n",
        "        if not is_hidden(file.name):\n",
        "          load_dataset(os.path.join(DATASET_LOCATION,file.name),file.name)\n",
        "\n",
        "def clean_data(x):\n",
        "  _ = []\n",
        "  for dataset in DATASETS[x]:\n",
        "    DATASETS[x][dataset].columns = [\"domain\"]\n",
        "    for i,c in DATASETS[x][dataset].iterrows():\n",
        "      if not is_parsed(c['domain']):\n",
        "        try:\n",
        "          get_parsed_domain(c['domain'])\n",
        "          characters_in_domain(get_second_level_domain(c['domain']))\n",
        "          _.append([get_second_level_domain(c['domain']),x , dataset])\n",
        "        except:\n",
        "          print(c['domain'])\n",
        "  return pd.DataFrame( _, columns=['domain','domain_class','agd_type'])\n",
        "\n",
        "def clean_words(x):\n",
        "  _ = []\n",
        "  for dataset in DATASETS[x]:\n",
        "    for i,c in DATASETS[x][dataset].iterrows():\n",
        "      if len(c['word']) >= 4:\n",
        "        characters_in_domain(c['word'])\n",
        "        _.append(c['word'])\n",
        "\n",
        "  return pd.DataFrame( _, columns=['word'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmJMalbtBiV7"
      },
      "source": [
        "find_and_load_dataset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwRowBHnBk5-"
      },
      "source": [
        "CLEANED_BENIGN_DATASET = clean_data('BENIGN')\n",
        "CLEANED_WORD_DATASET = clean_words('EN_WORDS')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXu-gMc2BpGl"
      },
      "source": [
        "def pre_processing():\n",
        "  for index, row in CLEANED_BENIGN_DATASET.iterrows():\n",
        "    add_to_distribution(row['domain'],CHARACTER_DISTRIBUTION_ENGLISH_DOMAINS)\n",
        "    add_grams(row['domain'],True)\n",
        "\n",
        "  for index,row in CLEANED_WORD_DATASET.iterrows():\n",
        "    add_to_distribution(row['word'],CHARACTER_DISTRIBUTION_ENGLISH_WORDS)\n",
        "    add_grams(row['word'],False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0DI19EMXBtJs"
      },
      "source": [
        "pre_processing()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4kyolFUBv_b"
      },
      "source": [
        "for x,y in CHARACTER_DISTRIBUTION_ENGLISH_DOMAINS.items():\n",
        "  if not x.isalpha():\n",
        "    CHARACTER_DISTRIBUTION_ENGLISH_WORDS[x] = CHARACTER_DISTRIBUTION_ENGLISH_DOMAINS[x]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wk1B88AbBxsw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "472a4c5c-97b0-48d1-e445-5fa9cd5e36b5"
      },
      "source": [
        "classifier.predict_proba(pd.DataFrame(extract_features('facebook')).values.reshape(1,16) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0024, 0.9976]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWKUvr0aUtIe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "897a3eee-6d2c-46a7-b628-2ca566fc4bdb"
      },
      "source": [
        "classifier.predict_proba(pd.DataFrame(extract_features('sdasdfafg')).values.reshape(1,16) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.9632, 0.0368]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TbeX2Z_yplf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9ec5466-e8ad-499d-8645-341af899c8f7"
      },
      "source": [
        "classifier.predict_proba(pd.DataFrame(extract_features('vitalpointz')).values.reshape(1,16) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0048, 0.9952]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpRrUEIOYmRs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ca9eaa80-92e5-4dcb-c318-686e1e1be37a"
      },
      "source": [
        "classifier.predict_proba(pd.DataFrame(extract_features('happy923eguyfdgk4life')).values.reshape(1,16) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.726, 0.274]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeuKmfMKMKFx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e9d0daa4-9ed8-457a-f4d7-3b744f332550"
      },
      "source": [
        "print('Alive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Alive\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}